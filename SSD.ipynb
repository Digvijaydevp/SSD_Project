{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83b61a-fffe-491c-b30a-4601f9cf24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586262a-0bea-4a54-9145-cf23da222726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet by removing URLs and special characters\n",
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters (keep letters, numbers, and spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and strip leading/trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd7286-5f57-46d0-918d-626fb253fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym replacement for sarcasm augmentation\n",
    "def get_synonyms(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb635f17-1983-4fbb-8f3b-c5a2412628f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_sentence = words.copy()\n",
    "    random_words = list(set([word for word in words if word not in ('CLS', 'SEP')]))\n",
    "    random.shuffle(random_words)\n",
    "\n",
    "    num_replacements = min(n, len(random_words))\n",
    "    for random_word in random_words[:num_replacements]:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_sentence = [synonym if word == random_word else word for word in new_sentence]\n",
    "    return ' '.join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fae54-ee43-48ae-876c-49028a733c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "file_path = './Tweets.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['text'] = df['text'].apply(clean_tweet)  # Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcba8bf-3385-4728-b75a-4e92ea16a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d42761-c712-4000-bf2f-87c19b63ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiments to numerical labels\n",
    "sentiment_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "df['label'] = df['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfa909-4018-4bc7-b7ca-9054550f3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len, augment=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Apply sarcasm-specific augmentation (synonym replacement)\n",
    "        if self.augment and random.uniform(0, 1) > 0.5:\n",
    "            text = synonym_replacement(text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e01a3-bde8-406e-8007-418e7a254e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61fdd2-3eda-4a10-acd3-0c77dda17a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects with sarcasm augmentation in the training set\n",
    "train_dataset = TweetDataset(train_texts, train_labels, tokenizer, MAX_LEN, augment=True)\n",
    "val_dataset = TweetDataset(val_texts, val_labels, tokenizer, MAX_LEN, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fecdc-d402-4a55-9db0-0273d510a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6e5d2-7d75-44fb-bccf-47e9496d5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07a4d8-15f1-407d-b0ab-a6079c051779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133fa8c-4976-4a4d-a256-a1e2473df8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251abb7c-0553-4ebd-a033-18097669f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        # Log detailed batch information\n",
    "        print(f'Epoch {epoch + 1}, Batch {batch_index + 1}/{len(data_loader)}:')\n",
    "        print(f'Input IDs:\\n{input_ids}')\n",
    "        print(f'Attention Mask:\\n{attention_mask}')\n",
    "        print(f'Labels:\\n{labels}')\n",
    "        print(f'Predicted Labels:\\n{preds}')\n",
    "        print(f'Loss: {loss.item():.4f}')\n",
    "        print('-' * 50)  # Add a separator for clarity between batches\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c324f3f-58d5-4b67-958d-691ba5d8abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get the predictions\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e703843-d7f5-4429-b14f-f7cb33fe4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3  # Number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    \n",
    "    # Train model with detailed batch information\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    print(f'Training loss: {train_loss}')\n",
    "\n",
    "    # Evaluate model\n",
    "    val_loss, val_accuracy = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_accuracy}')\n",
    "    print('=' * 50)  # Add separator between epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c53da3-daaa-4d60-87d0-56d05722e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned BERT model\n",
    "model.save_pretrained(\"bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db0b20-09ae-4e25-8c9a-3f8ad86475fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenizer, max_len=128):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    return encoding['input_ids'], encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312630d-38eb-4996-9c4b-d09cf48cc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Preprocess the text\n",
    "    input_ids, attention_mask = preprocess_text(text, tokenizer)\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get the prediction (the class with the highest logit score)\n",
    "    _, prediction = torch.max(outputs.logits, dim=1)\n",
    "    \n",
    "    # Convert prediction to CPU and return it\n",
    "    return prediction.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb43d4-d3c7-4136-a055-7b2e2e6a34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert_sentiment_model\")\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Load the tokenizer (same tokenizer used during training)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Test with a sample tweet\n",
    "sample_text = \"I love using BERT for NLP tasks!\"\n",
    "predicted_label = predict_sentiment(sample_text, model, tokenizer, device)\n",
    "\n",
    "# Map the predicted label back to sentiment\n",
    "label_mapping = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "print(f'Sentiment: {label_mapping[predicted_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12844e0e-ab02-4eb8-88d1-37ce13384766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiments(texts, model, tokenizer, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    for text in texts:\n",
    "        input_ids, attention_mask = preprocess_text(text, tokenizer)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, prediction = torch.max(outputs.logits, dim=1)\n",
    "            predictions.append(prediction.cpu().item())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Test with multiple sample tweets\n",
    "sample_texts = [\n",
    "    \"BERT is amazing for sentiment analysis!\",\n",
    "    \"I'm not sure about this.\",\n",
    "    \"I really hate it when things don't work.\"\n",
    "]\n",
    "predicted_labels = predict_sentiments(sample_texts, model, tokenizer, device)\n",
    "\n",
    "# Map predicted labels to sentiments\n",
    "predicted_sentiments = [label_mapping[label] for label in predicted_labels]\n",
    "for text, sentiment in zip(sample_texts, predicted_sentiments):\n",
    "    print(f'Tweet: {text}\\nSentiment: {sentiment}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098be07b-976d-4f6b-89b3-cc43c660ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert_sentiment_model\")\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Load the tokenizer (same tokenizer used during training)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Test with a sample tweet\n",
    "sample_text = \"Oh great, another rainy day! Just what I needed to brighten up my week. I was running out of excuses to stay indoors and do absolutely nothing! Who needs sunshine anyway?\"\n",
    "predicted_label = predict_sentiment(sample_text, model, tokenizer, device)\n",
    "\n",
    "# Map the predicted label back to sentiment\n",
    "label_mapping = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "print(f'Sentiment: {label_mapping[predicted_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a6da9-f9e7-4dd1-83dc-fd45a3d2753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert_sentiment_model\")\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Load the tokenizer (same tokenizer used during training)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Test with a sample tweet\n",
    "sample_text = \"Oh great, another Monday! Just what I needed to brighten my day. Can’t wait to dive into all this work. Sigh.\"\n",
    "\n",
    "# Map the predicted label back to sentiment\n",
    "label_mapping = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "print(f'Sentiment: {label_mapping[predicted_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ea3ff-cc2f-4ed2-9bad-ee0cab1a8866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
